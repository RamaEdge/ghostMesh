# THE-166: AI Explainer Service Testing Guide

## Overview

This directory contains comprehensive tests for THE-166: AI Explainer Service implementation. The AI Explainer Service is responsible for generating human-readable explanations for security alerts generated by the anomaly detector.

## Test Structure

```
tests/the166/
â”œâ”€â”€ README.md                           # This file
â”œâ”€â”€ test_plan.md                        # Comprehensive test plan
â”œâ”€â”€ test_explanation_generation.py      # Unit tests for explanation generation
â”œâ”€â”€ test_mqtt_handling.py              # MQTT integration tests (planned)
â”œâ”€â”€ test_alert_pipeline.py             # End-to-end pipeline tests (planned)
â”œâ”€â”€ test_dashboard_integration.py      # Dashboard integration tests (planned)
â”œâ”€â”€ test_performance.py                # Performance tests (planned)
â”œâ”€â”€ test_security.py                   # Security tests (planned)
â””â”€â”€ test_e2e.py                        # End-to-end tests (planned)
```

## Quick Start

### Prerequisites
- Python 3.9+
- pytest
- All GhostMesh services running (optional for unit tests)

### Running Tests

#### Run All THE-166 Tests
```bash
make test-the166
```

#### Run Specific Test Files
```bash
# Unit tests only
python -m pytest tests/the166/test_explanation_generation.py -v

# With coverage
python -m pytest tests/the166/test_explanation_generation.py --cov=explainer --cov-report=html
```

#### Run Individual Test Cases
```bash
# Run specific test method
python -m pytest tests/the166/test_explanation_generation.py::TestExplanationGeneration::test_temperature_anomaly_explanation -v
```

## Test Categories

### 1. Unit Tests (`test_explanation_generation.py`)
**Status:** âœ… Implemented

Tests the core explanation generation functionality:
- Temperature anomaly explanations
- Pressure anomaly explanations  
- Speed anomaly explanations
- Vibration anomaly explanations
- Unknown signal handling
- Confidence score calculation
- Asset context generation
- Signal analysis
- JSON schema validation

**Example Test:**
```python
def test_temperature_anomaly_explanation(self):
    explanation = self.explainer._generate_explanation(
        "alert-001", "Press01", "Temperature", "high", 45.2, "Statistical anomaly detected"
    )
    assert "Temperature anomaly" in explanation['text']
    assert explanation['confidence'] >= 0.5
    assert "inspect_equipment" in explanation['recommendedActions']
```

### 2. MQTT Integration Tests (`test_mqtt_handling.py`)
**Status:** ðŸ”„ Planned

Tests MQTT message handling:
- Alert message parsing
- Invalid message format handling
- Connection error recovery
- Message publishing validation
- QoS level verification

### 3. Alert Pipeline Tests (`test_alert_pipeline.py`)
**Status:** ðŸ”„ Planned

Tests the complete alert processing pipeline:
- End-to-end alert processing
- Multiple concurrent alerts
- Alert with missing context
- High-frequency alert processing
- Service restart and recovery

### 4. Dashboard Integration Tests (`test_dashboard_integration.py`)
**Status:** ðŸ”„ Planned

Tests integration with the Streamlit dashboard:
- Explanation display in dashboard
- Real-time explanation updates
- Explanation formatting and styling
- User interaction with explanations

### 5. Performance Tests (`test_performance.py`)
**Status:** ðŸ”„ Planned

Tests performance requirements:
- Single alert processing time
- Batch alert processing
- Memory usage under load
- CPU usage optimization
- Concurrent request handling

### 6. Security Tests (`test_security.py`)
**Status:** ðŸ”„ Planned

Tests security aspects:
- MQTT authentication with valid credentials
- MQTT authentication with invalid credentials
- ACL permission validation
- Secure credential handling

### 7. End-to-End Tests (`test_e2e.py`)
**Status:** ðŸ”„ Planned

Tests complete data flow:
- Telemetry â†’ Anomaly Detection â†’ Alert â†’ Explanation â†’ Dashboard
- Multiple assets with different anomaly types
- Service failure and recovery scenarios
- Data consistency across services

## Test Data

### Sample Alert Input
```json
{
  "alertId": "alert-001",
  "assetId": "Press01",
  "signal": "Temperature",
  "severity": "high",
  "reason": "Statistical anomaly detected",
  "current": 45.2,
  "ts": "2025-09-13T12:00:00Z"
}
```

### Expected Explanation Output
```json
{
  "alertId": "alert-001",
  "text": "Temperature anomaly (45.2Â°C) detected on Press01 (hydraulic press). This could indicate overheating, cooling system failure, or thermal stress. Immediate inspection of cooling systems and equipment status is recommended.",
  "confidence": 0.87,
  "riskLevel": "high",
  "recommendedActions": ["inspect_equipment", "check_cooling_system", "monitor_temperature"],
  "ts": "2025-09-13T12:00:01Z"
}
```

## Test Environment Setup

### Local Development
```bash
# Install dependencies
pip install -r explainer/requirements.txt

# Run unit tests
python -m pytest tests/the166/test_explanation_generation.py -v
```

### Full Integration Testing
```bash
# Start all services
make start

# Wait for services to stabilize
sleep 10

# Run integration tests
python -m pytest tests/the166/ -v
```

### Docker Testing
```bash
# Build the explainer service
make build-explainer

# Start all services
make start

# Check service logs
make logs-explainer
```

## Performance Targets

| Metric | Target | Test Method |
|--------|--------|-------------|
| Single alert processing | < 2 seconds | `test_performance.py` |
| Batch processing (10 alerts) | < 10 seconds | `test_performance.py` |
| Memory usage | < 100MB | `test_performance.py` |
| CPU usage | < 50% | `test_performance.py` |
| Concurrent alerts | 100/minute | `test_load.py` |

## Success Criteria

### Functional Requirements
- [ ] All unit tests pass
- [ ] All integration tests pass
- [ ] End-to-end data flow works correctly
- [ ] Dashboard displays explanations properly

### Performance Requirements
- [ ] Response time targets met
- [ ] Load handling targets met
- [ ] Memory usage within limits
- [ ] No performance regressions

### Quality Requirements
- [ ] Code coverage > 80%
- [ ] No critical bugs
- [ ] Documentation complete
- [ ] Deployment successful

## Troubleshooting

### Common Issues

#### Import Errors
```bash
# Ensure explainer module is in Python path
export PYTHONPATH="${PYTHONPATH}:$(pwd)/explainer"
```

#### MQTT Connection Issues
```bash
# Check if MQTT broker is running
make status

# Check MQTT logs
make logs-mqtt
```

#### Test Failures
```bash
# Run with verbose output
python -m pytest tests/the166/test_explanation_generation.py -v -s

# Run with debugging
python -m pytest tests/the166/test_explanation_generation.py --pdb
```

### Debug Commands
```bash
# Check service status
make status

# Monitor MQTT topics
make monitor-mqtt

# Check explainer logs
make logs-explainer

# Test MQTT connectivity
make test-mqtt
```

## Continuous Integration

### GitHub Actions (Planned)
```yaml
name: THE-166 Tests
on: [push, pull_request]
jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2
      - name: Set up Python
        uses: actions/setup-python@v2
        with:
          python-version: 3.9
      - name: Install dependencies
        run: pip install -r explainer/requirements.txt
      - name: Run tests
        run: make test-the166
```

## Contributing

### Adding New Tests
1. Create test file in appropriate category
2. Follow naming convention: `test_<functionality>.py`
3. Add test to Makefile if needed
4. Update this README with test description

### Test Naming Convention
- Test files: `test_<functionality>.py`
- Test classes: `Test<Functionality>`
- Test methods: `test_<specific_behavior>`

### Code Coverage
```bash
# Generate coverage report
python -m pytest tests/the166/ --cov=explainer --cov-report=html

# View coverage report
open htmlcov/index.html
```

## References

- [THE-166 Test Plan](test_plan.md)
- [AI Explainer Documentation](../../docs/AI_Explainer.md)
- [GhostMesh Architecture](../../docs/Architecture.md)
- [Project README](../../docs/Project_README.md)
