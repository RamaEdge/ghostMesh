# THE-166: AI Explainer Service Test Plan

## Overview

This document outlines the comprehensive testing strategy for THE-166: AI Explainer Service implementation. The AI Explainer Service is responsible for generating human-readable explanations for security alerts generated by the anomaly detector.

## Test Objectives

### Primary Objectives
1. **Alert Processing:** Verify the service correctly processes alerts from the anomaly detector
2. **Explanation Generation:** Ensure explanations are generated with appropriate content and format
3. **MQTT Integration:** Validate proper MQTT subscription and publishing
4. **Performance:** Confirm the service meets performance requirements
5. **Integration:** Test end-to-end integration with the GhostMesh pipeline

### Secondary Objectives
1. **Error Handling:** Verify robust error handling and recovery
2. **Security:** Validate authentication and authorization
3. **Scalability:** Test performance under load
4. **Monitoring:** Ensure proper logging and observability

## Test Categories

### 1. Unit Tests

#### 1.1 Explanation Generation Logic
**Test File:** `tests/the166/test_explanation_generation.py`

**Test Cases:**
- [ ] Temperature anomaly explanation generation
- [ ] Pressure anomaly explanation generation
- [ ] Speed anomaly explanation generation
- [ ] Vibration anomaly explanation generation
- [ ] Multi-signal anomaly explanation
- [ ] Edge case handling (insufficient data)
- [ ] Confidence score calculation
- [ ] Risk level assessment

**Expected Results:**
- Explanations contain relevant technical details
- Confidence scores are within valid range (0.0-1.0)
- Risk levels are appropriately assigned
- Edge cases are handled gracefully

#### 1.2 MQTT Message Handling
**Test File:** `tests/the166/test_mqtt_handling.py`

**Test Cases:**
- [ ] Alert message parsing
- [ ] Invalid message format handling
- [ ] Connection error recovery
- [ ] Message publishing validation
- [ ] QoS level verification

**Expected Results:**
- Valid alert messages are processed correctly
- Invalid messages are rejected with appropriate logging
- Connection issues are handled with retry logic
- Published explanations follow correct schema

### 2. Integration Tests

#### 2.1 Alert Processing Pipeline
**Test File:** `tests/the166/test_alert_pipeline.py`

**Test Cases:**
- [ ] End-to-end alert processing
- [ ] Multiple concurrent alerts
- [ ] Alert with missing context
- [ ] High-frequency alert processing
- [ ] Service restart and recovery

**Expected Results:**
- Alerts are processed within acceptable time limits
- Multiple alerts are handled concurrently
- Service recovers gracefully from failures
- No data loss during processing

#### 2.2 Dashboard Integration
**Test File:** `tests/the166/test_dashboard_integration.py`

**Test Cases:**
- [ ] Explanation display in dashboard
- [ ] Real-time explanation updates
- [ ] Explanation formatting and styling
- [ ] User interaction with explanations

**Expected Results:**
- Explanations appear in dashboard alerts table
- Real-time updates work correctly
- Formatting is consistent and readable
- User interactions function properly

### 3. Performance Tests

#### 3.1 Response Time Testing
**Test File:** `tests/the166/test_performance.py`

**Test Cases:**
- [ ] Single alert processing time
- [ ] Batch alert processing
- [ ] Memory usage under load
- [ ] CPU usage optimization
- [ ] Concurrent request handling

**Performance Targets:**
- Single alert processing: < 2 seconds
- Batch processing (10 alerts): < 10 seconds
- Memory usage: < 100MB under normal load
- CPU usage: < 50% under normal load

#### 3.2 Load Testing
**Test File:** `tests/the166/test_load.py`

**Test Cases:**
- [ ] High-frequency alert generation
- [ ] Sustained load over time
- [ ] Memory leak detection
- [ ] Service stability under stress

**Load Targets:**
- Process 100 alerts/minute
- Maintain stability for 1 hour continuous operation
- No memory leaks over extended periods

### 4. Security Tests

#### 4.1 Authentication Testing
**Test File:** `tests/the166/test_security.py`

**Test Cases:**
- [ ] MQTT authentication with valid credentials
- [ ] MQTT authentication with invalid credentials
- [ ] ACL permission validation
- [ ] Secure credential handling

**Expected Results:**
- Valid credentials allow proper access
- Invalid credentials are rejected
- ACL permissions are enforced
- Credentials are not exposed in logs

### 5. End-to-End Tests

#### 5.1 Complete Data Flow
**Test File:** `tests/the166/test_e2e.py`

**Test Cases:**
- [ ] Telemetry → Anomaly Detection → Alert → Explanation → Dashboard
- [ ] Multiple assets with different anomaly types
- [ ] Service failure and recovery scenarios
- [ ] Data consistency across services

**Expected Results:**
- Complete data flow works end-to-end
- All services integrate correctly
- Data consistency is maintained
- System recovers from failures

## Test Data

### Sample Alert Data
```json
{
  "alertId": "alert-001",
  "assetId": "Press01",
  "signal": "Temperature",
  "severity": "high",
  "reason": "Statistical anomaly detected",
  "current": 45.2,
  "ts": "2025-09-13T12:00:00Z"
}
```

### Expected Explanation Output
```json
{
  "alertId": "alert-001",
  "text": "High temperature detected on Press01 (45.2°C) indicates potential overheating. This could lead to equipment failure or safety hazards. Immediate inspection of cooling systems and equipment status is recommended.",
  "confidence": 0.87,
  "riskLevel": "high",
  "recommendedActions": ["inspect_equipment", "check_cooling_system", "monitor_temperature"],
  "ts": "2025-09-13T12:00:01Z"
}
```

## Test Environment Setup

### Prerequisites
- All GhostMesh services running (mosquitto, anomaly detector, dashboard)
- Test data generation capability
- Performance monitoring tools
- Log aggregation setup

### Test Execution Order
1. **Unit Tests:** Run first to validate core functionality
2. **Integration Tests:** Verify service interactions
3. **Performance Tests:** Validate performance requirements
4. **Security Tests:** Ensure security compliance
5. **End-to-End Tests:** Validate complete system functionality

## Test Automation

### Continuous Integration
- Unit tests run on every commit
- Integration tests run on pull requests
- Performance tests run nightly
- Security tests run weekly

### Test Reporting
- Test results published to CI dashboard
- Performance metrics tracked over time
- Security scan results reported
- Coverage reports generated

## Success Criteria

### Functional Requirements
- [ ] All unit tests pass
- [ ] All integration tests pass
- [ ] End-to-end data flow works correctly
- [ ] Dashboard displays explanations properly

### Performance Requirements
- [ ] Response time targets met
- [ ] Load handling targets met
- [ ] Memory usage within limits
- [ ] No performance regressions

### Security Requirements
- [ ] Authentication working correctly
- [ ] ACL permissions enforced
- [ ] No security vulnerabilities
- [ ] Secure credential handling

### Quality Requirements
- [ ] Code coverage > 80%
- [ ] No critical bugs
- [ ] Documentation complete
- [ ] Deployment successful

## Test Execution Commands

### Run All Tests
```bash
make test-the166
```

### Run Specific Test Categories
```bash
# Unit tests only
python -m pytest tests/the166/test_explanation_generation.py -v

# Integration tests only
python -m pytest tests/the166/test_alert_pipeline.py -v

# Performance tests only
python -m pytest tests/the166/test_performance.py -v

# End-to-end tests only
python -m pytest tests/the166/test_e2e.py -v
```

### Test with Coverage
```bash
python -m pytest tests/the166/ --cov=explainer --cov-report=html
```

## Troubleshooting

### Common Issues
1. **MQTT Connection Failures:** Check broker status and credentials
2. **Explanation Generation Timeouts:** Verify service performance
3. **Dashboard Integration Issues:** Check MQTT topic subscriptions
4. **Performance Degradation:** Monitor resource usage

### Debug Commands
```bash
# Check service logs
make logs-explainer

# Monitor MQTT topics
make monitor-mqtt

# Check service status
make status

# Performance monitoring
make monitor-performance
```

## Test Results Documentation

### Test Report Template
- Test execution summary
- Pass/fail statistics
- Performance metrics
- Security scan results
- Recommendations for improvements

### Metrics Tracking
- Test execution time
- Success/failure rates
- Performance benchmarks
- Coverage percentages
- Security vulnerability counts
